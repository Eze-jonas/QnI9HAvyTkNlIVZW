{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a6a312-da37-4188-8d17-1799742e2d34",
   "metadata": {},
   "source": [
    "#### Project Summary.\n",
    "The project: **Intelligent product marketing strategy using machine learning**, developed machine learning models to predict customer who will subscribe for a product(term deposit) using a European bank’s marketing dataset which was generated from their call campaign. The main target here is to improve the efficiency of randon search marking strategyy and identify customers mostly likely to subscribe.\n",
    "\n",
    "The dataset is strutured: 40,000, 14 and distributed : 0 = 37104, 1 = 2896 (92.8%, 7.2%). Seven(7) differnt notebooks were used including exploratory data analysis(EDA),pre campaign model preprocessing pipeline, pre campaign model development, post campaign model preprocessing pipeline, post campaign model development, customer segmentation and project summary( this notebook). \n",
    "#### Exploratory Data Analysis (EDA)\n",
    "During EDA,the data types of numerical variables were converted from integers to floats. New features were created, and the linear relationships and distributions of the variables were examined. Skewness was detected and handled using log and Yeo-Johnson transformation methods. Outliers were identified and treated using the Winsorization method. Numerical features were normalized using the Z-score standardization method, and categorical variables were encoded.\n",
    "#### Data Preprocessing Pipeline Development.\n",
    "The pipeline was developed based on the findings during EDA.\n",
    "It automates preprocessing of the dataset by applying the required transformations in the correct order.\n",
    "During EDA, necessary transformations were identified.\n",
    "Instead of repeating these steps manually every time, the pipeline ensures consistency and reproducibility.\n",
    "This prevents mistakes such as applying transformations in the wrong order or forgetting a step.\n",
    "It also makes the workflow dynamic and reusable, so the same pipeline can be applied to new data in the future.\n",
    "#### Pre-campaign Model Development.\n",
    "The pre-campaign model was built by dropping campaign-related features (e.g., call-related attributes) to prevent data leakage. Three different machine learning models — XGBoost, LightGBM, and Random Forest — were tuned, trained and evaluated for model selection. These models were chosen due to the high class imbalance and weak predictive power of the available features. The main purpose of this model is to improve the efficiency of the previously random search.\n",
    "\n",
    "* The dataset was split into 70% for training, 15% for validation, and 15% for testing. Manual class weights were applied to address the class imbalance issue. The baseline class weight (12.8) was computed from the training set, and additional values were tweaked around this baseline for experimentation and final selection.\n",
    "\n",
    "* Three random state seeds were auto-generated using random.randint(1000, 9999) and tested for stability.\n",
    "* 5-fold cross-validation within Hyperopt CV was employed for hyperparameter tuning, using a custom scoring metric that weighted precision (40%) and recall (60%). A total of 30 hyperparameter configurations were tested, and the best-performing candidate model was selected for final training.\n",
    "\n",
    "#### Model Training and Evaluation using Classification Report\n",
    "The best selected params, seed and class weight of each model were trained and evaluated using classification report and area under the curve (AUC)\n",
    "\n",
    "**Models Achievement on the test set and minority class**\n",
    "\n",
    "**XGBOOST** achieved efficiency of 8.65%, captured 81.57% potential subscribers  and reduced unnecessary calls by 12,753 (31.88%), saving approximately 902.7 hours of call time.\n",
    "\n",
    "**LGBM** achieved efficiency of 9.26%, captured 69.35% potential subscribers and reduced unnecessary calls by 18,169(45.42%), saving approximately 1.279.1 hours of call time.\n",
    "\n",
    "**Random Forest** achieved efficiency of 8.60%, captured 85.02% potential subscribers and reduced unnecessary calls by 11,098(27.74%), saving approximately 785.6 hours of call time.\n",
    "\n",
    "#### Feature Importance Extraction  and Identification of Segment Most Likely to Subscribe.\n",
    "In order to extract drivers of subscription and identify segment most likely to subsciber for more targeted campaign, **LGBM** model was trained and evaluated on the full dataset, **SHAP** values were computed and  analized. \n",
    "\n",
    "The **feature importance** were extracted using the shap values and visulized with bar chart while  shap summaries were used to visualize features that push the probability baseline up or down. \n",
    "\n",
    "#### Post Campaign Modele Development.\n",
    "This was developed with all the features including the campaign related features using LGBM model. The main purpose of this model is to intellegently identify customers most likely to subscribe to the term deposit to prevent calling those who never going to subscribe. \n",
    "* The full dataset was split into 70% for training, 15% for validation, and 15% for testing. Manual class weights were applied to address the class imbalance issue. The baseline class weight (12.8) was computed from the training set, and additional values were tweaked around this baseline for experimentation and final selection.\n",
    "* Three random state seeds were auto-generated using random.randint(1000, 9999) and tested for stability.\n",
    "* 5-fold cross-validation within Hyperopt CV was employed for hyperparameter tuning, using a custom scoring metric that weighted precision (40%) and recall (60%). A total of 30 hyperparameter configurations were tested, and the best-performing candidate model was selected for final training.\n",
    "\n",
    "**Model Achievement on test set and minority class.**\n",
    "The best pramenter combo was trained and evaluated using classification report and area under the curve(AUC).\n",
    "the achieved 48% precision, 72% Recall and AUC value of 0.99444\n",
    "\n",
    "#### customer Segmentation.\n",
    "Positive classes were extracted using DuckDB and segmented with k-means clustering to identify subscriber attributes and optimize marketing of additional investment products\n",
    "\n",
    "#### Conclusion / Recommendations.\n",
    "The application of intelligent modeling techniques to the product marketing campaign successfully improved the efficiency of the previously random search campaign through a pre-campaign model, identified customers most likely to subscribe via a post-campaign model, and provided the most effective strategies for marketing additional products through k-means clustering.\n",
    "\n",
    "* Integrating the pre-campaign LightGBM model into the company’s production system would enable the identification of 69.35% of potential subscribers, reduce unnecessary calls by 18,169 (45.42%), and save approximately 1,286.1 hours of call time. Similarly, integrating the post-campaign LightGBM model would ensure that 74% of contacted customers are those most likely to subscribe.\n",
    "\n",
    "* From the statistics of the top 20 features in clustering analysis\n",
    "\n",
    "**Cluster 0** consists of older customers aged between 41 and 70+. About 62% of them have a high account balance (greater than €407), while 37% have a low account balance (less than or equal to €407). 67% are married, 11% are single, and 21% are divorced. Around 12% are retired. Only 41% of this group showed interest in a housing loan.\n",
    "* Given their financial stability and life stage, this group is more inclined toward low-risk, income-generating, and capital-preserving products. Suitable offerings include term deposits, government or corporate bonds, retirement and pension plans, life or health insurance products, and personalized wealth management or estate planning services. Marketing strategies for this segment should emphasize financial security, stable income, and long-term comfort, rather than aggressive loan-based products.\n",
    "\n",
    "**Cluster 1** consists of younger customers aged approximately 19 to 41 years (with ages below 22 compressed to 22 due to winsorization). About 55% of them have a high account balance (greater than €407), while 45% have a low account balance (less than or equal to €407). 40% are married, 51% are single, and 0.8% are divorced. Around 0.1% are retired and 57% of this group showed interest in a housing loan.\n",
    "* Given their younger age, career stage, and growing financial activity, this group is more inclined toward moderate-risk, growth-oriented financial products that support wealth accumulation and lifestyle advancement. Suitable offerings include flexible savings plans, entry-level mutual funds or ETFs, home and personal loans, and retirement starter plans. Insurance products such as health and income protection plans are also relevant at this life stage. Marketing strategies for this segment should emphasize financial growth, independence, and convenience—highlighting digital banking solutions, automated investing, and opportunities to build assets early in life.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa0706-a9a7-4891-af7e-f980333597ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d07c74-bebc-46bf-8364-2c85074624f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
